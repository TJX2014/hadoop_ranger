https://openwebui.com/#open-webui-community

https://help.aliyun.com/zh/ecs/use-cases/use-intel-cpu-based-instances-to-deploy-the-qwen-1-8b-chat-recommendation-system-and-perform-a-query-rewriting-demonstration

git clone https://www.modelscope.cn/qwen/Qwen-1_8B-Chat.git
python -c 'import xfastertransformer as xft; xft.QwenConvert().convert("/mnt/data/Qwen-1_8B-Chat")'

docker run -d -p 3000:8080 -e WEBUI_AUTH=false -e OLLAMA_BASE_URL=http://0.0.0.0:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama

git clone https://github.com/ggerganov/llama.cpp.git
python convert_hf_to_gguf.py /Users/admin/mnt/data/Qwen-1_8B-Chat --outfile /Users/admin/mnt/data/Qwen-1_8B.gguf

cat Modelfile
```
FROM Qwen-1_8B.gguf

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 0.7
PARAMETER top_p 0.8
PARAMETER repeat_penalty 1.05
PARAMETER top_k 20

TEMPLATE """{{ if and .First .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}<|im_start|>user
{{ .Prompt }}<|im_end|>
<|im_start|>assistant
{{ .Response }}"""

# set the system message
SYSTEM """
You are a helpful assistant.
"""
```
nohup ollama serve > /tmp/ollama.log  &

ollama create qwen1_8B -f Modelfile

ollama list
ollama run qwen1_8B:latest


